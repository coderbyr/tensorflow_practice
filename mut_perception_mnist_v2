#coding:utf-8

from __future__ import print_function
import tensorflow as tf 
import input_data

##load mnist data
mnist = input_data.read_data_sets('../data/MNIST_data/', one_hot=True)

##parameters
n_hidden1 = 300
n_hidden2 = 200
n_input = 784
n_output = 10
batch_size = 100
display_step = 1

traing_epochs = 20

##input 
x = tf.placeholder(tf.float32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_output])

##create model
def multilayer_perception(x, weights, bias):
	layer1 = tf.add(tf.matmul(x, weights['w1']), bias['b1'])
	layer1 = tf.nn.relu(layer1)

	layer2 = tf.add(tf.matmul(layer1, weights['w2']), bias['b2'])
	layer2 = tf.nn.relu(layer2)

	output_layer = tf.add(tf.matmul(layer2, weights['w3']), bias['b3'])
	#output_layer = tf.nn.softmax(output_layer)
	return output_layer

def get_weights(n_input, n_hidden1, n_hidden2, n_output):
	weights = {}
	weights['w1'] = tf.Variable(tf.truncated_normal([n_input, n_hidden1], stddev=0.1))
	weights['w2'] = tf.Variable(tf.random_normal([n_hidden1, n_hidden2]), dtype=tf.float32)
	weights['w3'] = tf.Variable(tf.random_normal([n_hidden2, n_output]), dtype=tf.float32)

	return weights

def get_bias(n_input, n_hidden1, n_hidden2, n_output):
	bias = {}
	bias['b1'] = tf.Variable(tf.random_normal([n_hidden1]), dtype=tf.float32)
	bias['b2'] = tf.Variable(tf.random_normal([n_hidden2]), dtype=tf.float32)
	bias['b3'] = tf.Variable(tf.random_normal([n_output]), dtype=tf.float32)
	return bias

weights = get_weights(n_input, n_hidden1, n_hidden2, n_output)
bias = get_bias(n_input, n_hidden1, n_hidden2, n_output)
y_ = multilayer_perception(x, weights, bias)


##loss function
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_))

#train
train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)

#run
init = tf.global_variables_initializer()
with tf.Session() as sess:
	sess.run(init)
	for epoch in range(traing_epochs):
		avg_cost = 0.0
		total_batch = int(mnist.train.num_examples/batch_size)

		for i in range(total_batch):
			batch_xs, batch_ys = mnist.train.next_batch(batch_size)
			_, c = sess.run([train_step, cost], feed_dict={x:batch_xs, y:batch_ys})
			avg_cost += c / total_batch
		if epoch % display_step == 0:
			print("Epoch:", '%04d' % (epoch+1), "cost=", \
             	"{:.9f}".format(avg_cost))
	print("Optimization Finished!")


	#prediction
	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	print(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))
